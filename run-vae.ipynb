{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.framework import arg_scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as nets\n",
    "import tensorflow as tf\n",
    "\n",
    "from plot import make_canvas, make_spread\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE:\n",
    "\n",
    "    def __init__(self, latent_dim, batch_size, encoder, decoder):\n",
    "        \"\"\"\n",
    "        Implementation of Variational Autoencoder (VAE) for  MNIST.\n",
    "        Paper (Kingma & Welling): https://arxiv.org/abs/1312.6114.\n",
    "\n",
    "        :param latent_dim: Dimension of latent space.\n",
    "        :param batch_size: Number of data points per mini batch.\n",
    "        :param encoder: function which encodes a batch of inputs to a \n",
    "            parameterization of a diagonal Gaussian\n",
    "        :param decoder: function which decodes a batch of samples from \n",
    "            the latent space and returns the corresponding batch of images.\n",
    "        \"\"\"\n",
    "        self._latent_dim = latent_dim\n",
    "        self._batch_size = batch_size\n",
    "        self._encode = encoder\n",
    "        self._decode = decoder\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Build tensorflow computational graph for VAE.\n",
    "        x -> encode(x) -> latent parameterization & KL divergence ->\n",
    "        z -> decode(z) -> distribution over x -> log likelihood ->\n",
    "        total loss -> train step\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('vae'):\n",
    "            # placeholder for MNIST inputs\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 28 * 28])\n",
    "\n",
    "            # encode inputs (map to parameterization of diagonal Gaussian)\n",
    "            with tf.variable_scope('encoder'):\n",
    "                # use relu activations\n",
    "                with arg_scope([layers.fully_connected,\n",
    "                                layers.conv2d, layers.conv2d_transpose],\n",
    "                               activation_fn=tf.nn.relu):\n",
    "                    self.encoded = self._encode(self.x, self._latent_dim)\n",
    "\n",
    "            with tf.variable_scope('sampling'):\n",
    "                # extract mean and (diagonal) log variance of latent variable\n",
    "                self.mean = self.encoded[:, :self._latent_dim]\n",
    "                self.logvar = self.encoded[:, self._latent_dim:]\n",
    "                # also calculate standard deviation for practical use\n",
    "                self.stddev = tf.sqrt(tf.exp(self.logvar))\n",
    "\n",
    "                # sample from latent space\n",
    "                epsilon = tf.random_normal([self._batch_size, self._latent_dim])\n",
    "                self.z = self.mean + self.stddev * epsilon\n",
    "\n",
    "            # decode batch\n",
    "            with tf.variable_scope('decoder'):\n",
    "                # use relu activations\n",
    "                with arg_scope([layers.fully_connected,\n",
    "                                layers.conv2d, layers.conv2d_transpose],\n",
    "                               activation_fn=tf.nn.relu):\n",
    "                    self.decoded = self._decode(self.z)\n",
    "\n",
    "            with tf.variable_scope('loss'):\n",
    "                # calculate KL divergence between approximate posterior q and prior p\n",
    "                with tf.variable_scope('kl-divergence'):\n",
    "                    kl = self._kl_diagnormal_stdnormal(self.mean, self.stddev)\n",
    "\n",
    "                # calculate reconstruction error between decoded sample\n",
    "                # and original input batch\n",
    "                with tf.variable_scope('log-likelihood'):\n",
    "                    log_like = self._bernoulli_log_likelihood(self.x, self.decoded)\n",
    "\n",
    "                self._loss = (kl + log_like) / self._batch_size\n",
    "\n",
    "            with tf.variable_scope('optimizer'):\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=2e-4)\n",
    "            with tf.variable_scope('training-step'):\n",
    "                self._train = optimizer.minimize(self._loss)\n",
    "\n",
    "            # start tensorflow session\n",
    "            self._sesh = tf.Session()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self._sesh.run(init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _kl_diagnormal_stdnormal(mu, sigma, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Calculates KL Divergence between q~N(mu, sigma^T * I) and p~N(0, I).\n",
    "        q(z|x) is the approximate posterior over the latent variable z,\n",
    "        and p(z) is the prior on z.\n",
    "\n",
    "        :param mu: Mean of z under approximate posterior.\n",
    "        :param sigma: Standard deviation of z\n",
    "            under approximate posterior.\n",
    "        :param eps: Small value to prevent log(0).\n",
    "        :return: kl: KL Divergence between q(z|x) and p(z).\n",
    "        \"\"\"\n",
    "        var = tf.square(sigma)\n",
    "        kl = 0.5 * tf.reduce_sum(tf.square(mu) + var - 1. - tf.log(var + eps))\n",
    "        return kl\n",
    "\n",
    "    @staticmethod\n",
    "    def _bernoulli_log_likelihood(targets, outputs, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Calculates negative log likelihood -log(p(x|z)) of outputs,\n",
    "        assuming a Bernoulli distribution.\n",
    "\n",
    "        :param targets: MNIST images.\n",
    "        :param outputs: Probability distribution over outputs.\n",
    "        :return: log_like: -log(p(x|z)) (negative log likelihood)\n",
    "        \"\"\"\n",
    "        log_like = -tf.reduce_sum(targets * tf.log(outputs + eps)\n",
    "                                  + (1. - targets) * tf.log((1. - outputs) + eps))\n",
    "        return log_like\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        Performs one mini batch update of parameters for both inference\n",
    "        and generative networks.\n",
    "\n",
    "        :param x: Mini batch of input data points.\n",
    "        :return: loss: Total loss (KL + NLL) for mini batch.\n",
    "        \"\"\"\n",
    "        _, loss = self._sesh.run([self._train, self._loss],\n",
    "                                 feed_dict={self.x: x})\n",
    "        return loss\n",
    "\n",
    "    def x2z(self, x):\n",
    "        \"\"\"\n",
    "        Maps a data point x (i.e. an image) to mean in latent space.\n",
    "\n",
    "        :return: mean: mu such that q(z|x) ~ N(mu, .).\n",
    "        \"\"\"\n",
    "        mean = self._sesh.run([self.mean], feed_dict={self.x: x})\n",
    "        return np.asarray(mean).reshape(-1, self._latent_dim)\n",
    "\n",
    "    def z2x(self, z):\n",
    "        \"\"\"\n",
    "        Maps a point in latent space to a 28*28 image.\n",
    "\n",
    "        :param z: Point in latent space.\n",
    "        :return: x: Corresponding image generated from z.\n",
    "        \"\"\"\n",
    "        x = self._sesh.run([self.decoded],\n",
    "                           feed_dict={self.z: z})\n",
    "        # need to reshape since our network processes batches of 1-D 28 * 28 arrays\n",
    "        x = np.array(x)[:, 0, :].reshape(28, 28)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
